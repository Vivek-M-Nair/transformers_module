{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67500c3-4051-49c3-b59a-d3bf48c52fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae6516ef-92f6-49b1-9b7d-8d9e87fd7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    " # !pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcb028e-caf2-45d7-9fe4-1c1bf605793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis de Ktm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name=\"google/flan-t5-large\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "input_text = \"Translate 'I am from Ktm' to French:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07635b4c-1e5c-4a1a-a179-6438a869afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcutta\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is the capital of India?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13379001-f471-4e6d-96a2-63a15379ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning is my favorite subject.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Complete the sentance: i like Learning and\"\n",
    "input= tokenizer(input_text, return_tensors= \"pt\")\n",
    "outputs=model.generate(**input)\n",
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896fd6bd-99bb-4645-a9c1-15a2790c9bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understand the importance of tokenization.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Summarize : Tokenization is an important step in many NLP tasks, such as text classification, machine translation, and question answering. Tokenization helps machines understand human language, making it easier for them to analyze. \"\n",
    "inputs=tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs=model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
